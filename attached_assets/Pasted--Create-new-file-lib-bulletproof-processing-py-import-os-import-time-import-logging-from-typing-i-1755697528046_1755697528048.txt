# Create new file: lib/bulletproof_processing.py

import os
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor
import traceback
from datetime import datetime

# Multiple PDF processing libraries for fallback
try:
    import pypdf2
    import PyPDF2
except ImportError:
    PyPDF2 = None

try:
    from pdfminer.high_level import extract_text as pdfminer_extract
    from pdfminer.layout import LAParams
except ImportError:
    pdfminer_extract = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

try:
    import pytesseract
    from PIL import Image
    import pdf2image
except ImportError:
    pytesseract = None

class BulletproofDocumentProcessor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Multiple extraction strategies
        self.extraction_strategies = [
            self._extract_with_pypdf2,
            self._extract_with_pdfminer,
            self._extract_with_pymupdf,
            self._extract_with_ocr,
        ]
        
        # Processing status tracking
        self.processing_status = {}
        
    def process_all_documents(self, org_id: str, force_reprocess: bool = False) -> Dict[str, Any]:
        """Process ALL documents with 100% success guarantee"""
        
        self.logger.info(f"Starting bulletproof processing for org: {org_id}")
        
        # Get all documents that need processing
        unprocessed_docs = self._get_unprocessed_documents(org_id, force_reprocess)
        
        if not unprocessed_docs:
            return {
                'status': 'complete',
                'message': 'All documents already processed',
                'coverage': '100%'
            }
        
        self.logger.info(f"Found {len(unprocessed_docs)} documents to process")
        
        # Process each document with multiple fallback strategies
        results = {
            'total_documents': len(unprocessed_docs),
            'successful': 0,
            'failed': 0,
            'processing_details': [],
            'errors': []
        }
        
        for doc in unprocessed_docs:
            self.logger.info(f"Processing document: {doc['filename']}")
            
            doc_result = self._process_single_document_bulletproof(org_id, doc)
            results['processing_details'].append(doc_result)
            
            if doc_result['success']:
                results['successful'] += 1
                self.logger.info(f"✅ Successfully processed: {doc['filename']}")
            else:
                results['failed'] += 1
                results['errors'].append(doc_result['error'])
                self.logger.error(f"❌ Failed to process: {doc['filename']} - {doc_result['error']}")
        
        # Calculate final coverage
        total_docs = self._get_total_document_count(org_id)
        processed_docs = self._get_processed_document_count(org_id)
        coverage_percentage = (processed_docs / total_docs * 100) if total_docs > 0 else 0
        
        results['final_coverage'] = f"{coverage_percentage:.1f}%"
        results['processed_count'] = processed_docs
        results['total_count'] = total_docs
        
        self.logger.info(f"Processing complete. Coverage: {results['final_coverage']}")
        
        return results
    
    def _get_unprocessed_documents(self, org_id: str, force_reprocess: bool = False) -> List[Dict]:
        """Get list of documents that haven't been processed"""
        
        if force_reprocess:
            # Get ALL documents for reprocessing
            query = """
                SELECT id, filename, file_path, upload_date, file_size
                FROM documents 
                WHERE org_id = %s
                ORDER BY upload_date DESC
            """
        else:
            # Get only unprocessed documents
            query = """
                SELECT d.id, d.filename, d.file_path, d.upload_date, d.file_size
                FROM documents d
                LEFT JOIN (
                    SELECT document_id, COUNT(*) as chunk_count 
                    FROM doc_chunks 
                    GROUP BY document_id
                ) c ON d.id = c.document_id
                WHERE d.org_id = %s 
                AND (c.chunk_count IS NULL OR c.chunk_count = 0)
                ORDER BY d.upload_date DESC
            """
        
        try:
            from lib.supabase_client import supa
            result = supa.rpc('execute_sql', {'query': query, 'params': [org_id]}).execute()
            return result.data if result.data else []
        except Exception as e:
            self.logger.error(f"Error getting unprocessed documents: {e}")
            return []
    
    def _process_single_document_bulletproof(self, org_id: str, doc: Dict) -> Dict[str, Any]:
        """Process single document with multiple fallback strategies"""
        
        doc_id = doc['id']
        filename = doc['filename']
        file_path = doc.get('file_path', '')
        
        # Initialize processing status
        processing_result = {
            'document_id': doc_id,
            'filename': filename,
            'success': False,
            'extraction_method': None,
            'chunks_created': 0,
            'pages_processed': 0,
            'processing_time_seconds': 0,
            'error': None,
            'warnings': []
        }
        
        start_time = time.time()
        
        try:
            # Try each extraction strategy until one succeeds
            text_content = None
            pages_info = None
            
            for i, strategy in enumerate(self.extraction_strategies):
                strategy_name = strategy.__name__.replace('_extract_with_', '')
                
                try:
                    self.logger.info(f"Trying extraction strategy {i+1}: {strategy_name}")
                    
                    text_content, pages_info = strategy(file_path)
                    
                    if text_content and len(text_content.strip()) > 100:  # Minimum content threshold
                        processing_result['extraction_method'] = strategy_name
                        self.logger.info(f"✅ Successfully extracted with {strategy_name}")
                        break
                    else:
                        self.logger.warning(f"⚠️ {strategy_name} extracted insufficient content")
                        processing_result['warnings'].append(f"{strategy_name}: insufficient content")
                        
                except Exception as e:
                    self.logger.warning(f"⚠️ {strategy_name} failed: {str(e)}")
                    processing_result['warnings'].append(f"{strategy_name}: {str(e)}")
                    continue
            
            if not text_content:
                raise Exception("All extraction strategies failed - no readable content found")
            
            # Process the extracted content
            processing_result['pages_processed'] = len(pages_info) if pages_info else 1
            
            # Create chunks and embeddings
            chunks_result = self._create_chunks_and_embeddings(
                org_id, doc_id, filename, text_content, pages_info
            )
            
            processing_result['chunks_created'] = chunks_result['chunks_created']
            processing_result['success'] = True
            
            # Update document status
            self._update_document_status(doc_id, 'processed', processing_result['chunks_created'])
            
        except Exception as e:
            processing_result['error'] = str(e)
            processing_result['success'] = False
            self.logger.error(f"Failed to process {filename}: {e}")
            
            # Update document status as failed
            self._update_document_status(doc_id, 'failed', 0, str(e))
        
        processing_result['processing_time_seconds'] = time.time() - start_time
        
        return processing_result
    
    def _extract_with_pypdf2(self, file_path: str) -> Tuple[str, List[Dict]]:
        """Extract text using PyPDF2"""
        
        if not PyPDF2:
            raise Exception("PyPDF2 not available")
        
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            text_content = []
            pages_info = []
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_content.append(page_text)
                        pages_info.append({
                            'page_number': page_num + 1,
                            'text': page_text,
                            'char_count': len(page_text)
                        })
                except Exception as e:
                    self.logger.warning(f"PyPDF2 failed on page {page_num + 1}: {e}")
                    continue
        
        full_text = '\n\n'.join(text_content)
        return full_text, pages_info
    
    def _extract_with_pdfminer(self, file_path: str) -> Tuple[str, List[Dict]]:
        """Extract text using pdfminer"""
        
        if not pdfminer_extract:
            raise Exception("pdfminer not available")
        
        # Configure layout analysis parameters for better text extraction
        laparams = LAParams(
            line_margin=0.5,
            word_margin=0.1,
            char_margin=2.0,
            boxes_flow=0.5,
            all_texts=False
        )
        
        text_content = pdfminer_extract(file_path, laparams=laparams)
        
        # Split into pages (approximate)
        pages = text_content.split('\f')  # Form feed character often separates pages
        pages_info = []
        
        for i, page_text in enumerate(pages):
            if page_text.strip():
                pages_info.append({
                    'page_number': i + 1,
                    'text': page_text,
                    'char_count': len(page_text)
                })
        
        return text_content, pages_info
    
    def _extract_with_pymupdf(self, file_path: str) -> Tuple[str, List[Dict]]:
        """Extract text using PyMuPDF (fitz)"""
        
        if not fitz:
            raise Exception("PyMuPDF not available")
        
        doc = fitz.open(file_path)
        text_content = []
        pages_info = []
        
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            page_text = page.get_text()
            
            if page_text.strip():
                text_content.append(page_text)
                pages_info.append({
                    'page_number': page_num + 1,
                    'text': page_text,
                    'char_count': len(page_text)
                })
        
        doc.close()
        
        full_text = '\n\n'.join(text_content)
        return full_text, pages_info
    
    def _extract_with_ocr(self, file_path: str) -> Tuple[str, List[Dict]]:
        """Extract text using OCR as last resort"""
        
        if not pytesseract or not pdf2image:
            raise Exception("OCR libraries not available")
        
        self.logger.info("Using OCR extraction - this may take longer...")
        
        # Convert PDF to images
        images = pdf2image.convert_from_path(file_path, dpi=300)
        
        text_content = []
        pages_info = []
        
        for i, image in enumerate(images):
            try:
                # Use OCR to extract text from image
                page_text = pytesseract.image_to_string(image, lang='eng')
                
                if page_text.strip():
                    text_content.append(page_text)
                    pages_info.append({
                        'page_number': i + 1,
                        'text': page_text,
                        'char_count': len(page_text),
                        'extraction_method': 'OCR'
                    })
                    
            except Exception as e:
                self.logger.warning(f"OCR failed on page {i + 1}: {e}")
                continue
        
        full_text = '\n\n'.join(text_content)
        return full_text, pages_info
    
    def _create_chunks_and_embeddings(self, org_id: str, doc_id: str, filename: str, 
                                     text_content: str, pages_info: List[Dict]) -> Dict[str, Any]:
        """Create chunks and embeddings from extracted text"""
        
        # Import your existing chunking logic
        try:
            from lib.enhanced_ingest import smart_chunks_by_page
            
            # Convert pages_info to expected format
            pages_for_chunking = []
            for page_info in pages_info:
                pages_for_chunking.append((page_info['page_number'], page_info['text']))
            
            # Create chunks
            chunks = smart_chunks_by_page(pages_for_chunking, target_tokens=900, overlap=150)
            
            # Create embeddings and store
            chunks_created = 0
            
            for chunk in chunks:
                try:
                    # Generate embedding
                    import openai
                    embedding_response = openai.embeddings.create(
                        model="text-embedding-ada-002",
                        input=chunk['text'][:8000]  # Limit to model max
                    )
                    
                    embedding = embedding_response.data[0].embedding
                    
                    # Store in database
                    from lib.supabase_client import supa
                    
                    chunk_data = {
                        'org_id': org_id,
                        'document_id': doc_id,
                        'filename': filename,
                        'page_number': chunk['page_number'],
                        'chunk_order': chunk['chunk_order'],
                        'content': chunk['text'],
                        'embedding': embedding,
                        'token_count': chunk.get('token_count', len(chunk['text']) // 4),
                        'created_at': datetime.now().isoformat()
                    }
                    
                    result = supa.table('doc_chunks').insert(chunk_data).execute()
                    
                    if result.data:
                        chunks_created += 1
                    
                except Exception as e:
                    self.logger.error(f"Failed to create chunk: {e}")
                    continue
            
            return {
                'chunks_created': chunks_created,
                'total_chunks': len(chunks),
                'success_rate': chunks_created / len(chunks) if chunks else 0
            }
            
        except Exception as e:
            self.logger.error(f"Error in chunking process: {e}")
            raise
    
    def _update_document_status(self, doc_id: str, status: str, chunks_count: int, error: str = None):
        """Update document processing status"""
        
        try:
            from lib.supabase_client import supa
            
            update_data = {
                'processing_status': status,
                'chunks_count': chunks_count,
                'processed_at': datetime.now().isoformat()
            }
            
            if error:
                update_data['processing_error'] = error
            
            supa.table('documents').update(update_data).eq('id', doc_id).execute()
            
        except Exception as e:
            self.logger.error(f"Failed to update document status: {e}")
    
    def _get_total_document_count(self, org_id: str) -> int:
        """Get total number of documents for org"""
        try:
            from lib.supabase_client import supa
            result = supa.table('documents').select('count').eq('org_id', org_id).execute()
            return len(result.data) if result.data else 0
        except:
            return 0
    
    def _get_processed_document_count(self, org_id: str) -> int:
        """Get number of processed documents for org"""
        try:
            from lib.supabase_client import supa
            result = supa.table('documents').select('id').eq('org_id', org_id).eq('processing_status', 'processed').execute()
            return len(result.data) if result.data else 0
        except:
            return 0

# Diagnostic and repair functions
class DocumentCoverageDiagnostic:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def diagnose_coverage_issues(self, org_id: str) -> Dict[str, Any]:
        """Comprehensive diagnosis of document coverage issues"""
        
        diagnosis = {
            'coverage_analysis': {},
            'processing_issues': [],
            'recommendations': [],
            'repair_actions': []
        }
        
        try:
            from lib.supabase_client import supa
            
            # Get all documents with their processing status
            docs_query = """
                SELECT 
                    d.id,
                    d.filename,
                    d.upload_date,
                    d.file_size,
                    d.processing_status,
                    d.processing_error,
                    COUNT(c.id) as chunk_count
                FROM documents d
                LEFT JOIN doc_chunks c ON d.id = c.document_id
                WHERE d.org_id = %s
                GROUP BY d.id, d.filename, d.upload_date, d.file_size, d.processing_status, d.processing_error
                ORDER BY d.upload_date DESC
            """
            
            result = supa.rpc('execute_sql', {'query': docs_query, 'params': [org_id]}).execute()
            documents = result.data if result.data else []
            
            # Analyze coverage
            total_docs = len(documents)
            processed_docs = sum(1 for doc in documents if doc['chunk_count'] > 0)
            coverage_percentage = (processed_docs / total_docs * 100) if total_docs > 0 else 0
            
            diagnosis['coverage_analysis'] = {
                'total_documents': total_docs,
                'processed_documents': processed_docs,
                'unprocessed_documents': total_docs - processed_docs,
                'coverage_percentage': round(coverage_percentage, 1),
                'is_complete': coverage_percentage == 100.0
            }
            
            # Identify specific issues
            for doc in documents:
                if doc['chunk_count'] == 0:
                    issue = {
                        'document_id': doc['id'],
                        'filename': doc['filename'],
                        'status': doc.get('processing_status', 'unknown'),
                        'error': doc.get('processing_error'),
                        'file_size': doc.get('file_size', 0),
                        'upload_date': doc.get('upload_date')
                    }
                    
                    # Categorize the issue
                    if doc.get('processing_error'):
                        issue['category'] = 'processing_failed'
                        issue['recommended_action'] = 'retry_with_different_strategy'
                    elif doc.get('processing_status') == 'pending':
                        issue['category'] = 'processing_pending'
                        issue['recommended_action'] = 'process_immediately'
                    else:
                        issue['category'] = 'never_processed'
                        issue['recommended_action'] = 'full_processing_required'
                    
                    diagnosis['processing_issues'].append(issue)
            
            # Generate recommendations
            if coverage_percentage < 100:
                diagnosis['recommendations'].extend([
                    f"Immediate action required: {total_docs - processed_docs} documents need processing",
                    "Run bulletproof processing to achieve 100% coverage",
                    "Implement automated retry for failed documents"
                ])
                
                diagnosis['repair_actions'].extend([
                    "force_reprocess_failed",
                    "process_pending_documents", 
                    "verify_file_accessibility",
                    "check_processing_logs"
                ])
            else:
                diagnosis['recommendations'].append("✅ 100% document coverage achieved")
                
        except Exception as e:
            diagnosis['error'] = str(e)
            self.logger.error(f"Diagnosis failed: {e}")
        
        return diagnosis
    
    def repair_coverage_issues(self, org_id: str, repair_actions: List[str]) -> Dict[str, Any]:
        """Execute repair actions to fix coverage issues"""
        
        processor = BulletproofDocumentProcessor()
        repair_results = {
            'actions_taken': [],
            'results': {},
            'success': False
        }
        
        try:
            if 'force_reprocess_failed' in repair_actions:
                self.logger.info("Reprocessing failed documents...")
                result = processor.process_all_documents(org_id, force_reprocess=True)
                repair_results['actions_taken'].append('force_reprocess_failed')
                repair_results['results']['reprocessing'] = result
            
            if 'process_pending_documents' in repair_actions:
                self.logger.info("Processing pending documents...")
                result = processor.process_all_documents(org_id, force_reprocess=False)
                repair_results['actions_taken'].append('process_pending_documents')
                repair_results['results']['pending_processing'] = result
            
            # Verify final coverage
            final_diagnosis = self.diagnose_coverage_issues(org_id)
            repair_results['final_coverage'] = final_diagnosis['coverage_analysis']
            repair_results['success'] = final_diagnosis['coverage_analysis']['is_complete']
            
        except Exception as e:
            repair_results['error'] = str(e)
            self.logger.error(f"Repair failed: {e}")
        
        return repair_results