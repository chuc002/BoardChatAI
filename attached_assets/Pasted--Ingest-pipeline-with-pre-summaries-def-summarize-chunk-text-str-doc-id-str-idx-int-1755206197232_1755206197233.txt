# --- Ingest pipeline (with pre-summaries) ---
def _summarize_chunk(text: str, doc_id: str, idx: int) -> str:
    """
    Compress a chunk to a compact, factual note ending with its citation.
    We cap length so it stays ~60–100 tokens.
    """
    from openai import OpenAI
    client = OpenAI()
    prompt = (
        "Condense the following board-document excerpt into 1–3 short bullet sentences, "
        "keeping dates, amounts, named entities, rules, and obligations. "
        "No preamble. End with the citation token exactly once.\n\n"
        f"EXCERPT [{doc_id}#{idx}]:\n{text[:1500]}\n\n"
        f"Finish with [Doc:{doc_id}#Chunk:{idx}]"
    )
    try:
        resp = client.chat.completions.create(
            model=os.getenv("CHAT_COMPRESS","gpt-4o-mini"),
            temperature=0.0,
            messages=[{"role":"user","content":prompt}],
            max_tokens=140  # ~100 tokens output + safety
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception:
        # fall back to truncation if API hiccups
        return (text[:500] + f"... [Doc:{doc_id}#Chunk:{idx}]")

def upsert_document(org_id: str, user_id: str, filename: str, file_bytes: bytes, mime_type: str) -> Tuple[dict, int]:
    sha = _sha256_bytes(file_bytes)
    storage_path = f"{org_id}/{sha}/{filename}"

    # De-dupe by sha256
    existing = supa.table("documents").select("*").eq("org_id", org_id).eq("sha256", sha).limit(1).execute().data
    if existing:
        supa.storage.from_(SUPABASE_BUCKET).upload(storage_path, file_bytes, {
            "content-type": mime_type,
            "x-upsert": "true"
        })
        return existing[0], 0

    # Upload binary
    supa.storage.from_(SUPABASE_BUCKET).upload(storage_path, file_bytes, {
        "content-type": mime_type,
        "x-upsert": "true"
    })

    # Insert document row
    doc = supa.table("documents").insert({
        "org_id": org_id,
        "created_by": user_id,
        "title": filename,
        "name": filename,
        "filename": filename,
        "storage_path": storage_path,
        "file_path": storage_path,
        "sha256": sha,
        "mime_type": mime_type,
        "size_bytes": len(file_bytes),
        "status": "processing",
        "processed": False,
        "processing_error": None
    }).execute().data[0]

    # Extract text
    text = extract_text_from_pdf(file_bytes)
    if len(text.strip()) < 50:
        supa.table("documents").update({
            "status": "error",
            "processed": False,
            "processing_error": "Scanned PDF not supported in MVP"
        }).eq("id", doc["id"]).execute()
        return doc, 0

    # Chunk + embed
    chunks = smart_chunks(text, target_tokens=900, overlap=150)
    embeddings = embed_texts(chunks)

    # Pre-summarize each chunk (budgeted; fast mini model)
    rows = []
    for i, (c, e) in enumerate(zip(chunks, embeddings)):
        s = _summarize_chunk(c, doc["id"], i)
        rows.append({
            "org_id": org_id,
            "document_id": doc["id"],
            "chunk_index": i,
            "content": c,
            "summary": s,             # <= we store the pre-summary
            "token_count": len(c),
            "embedding": e
        })

    if rows:
        supa.table("doc_chunks").insert(rows).execute()
        supa.table("documents").update({"status": "ready", "processed": True}).eq("id", doc["id"]).execute()
    else:
        supa.table("documents").update({"status": "error", "processed": False, "processing_error": "no chunks"}).eq("id", doc["id"]).execute()

    return doc, len(rows)
