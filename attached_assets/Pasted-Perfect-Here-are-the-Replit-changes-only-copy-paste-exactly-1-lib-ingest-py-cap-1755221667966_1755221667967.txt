Perfect. Here are the **Replit changes only** — copy/paste exactly.

---

# 1) `lib/ingest.py` — capture page numbers during ingest

**Replace your current extraction + chunking code** with the version below.
(It plugs into your existing `upsert_document` exactly.)

```python
# add near the top with imports if not present
import tiktoken

# --- PDF text extraction WITH PAGES ---
def extract_pages_from_pdf(file_bytes: bytes) -> list[tuple[int, str]]:
    """
    Returns a list of (page_index, text) using PyPDF; falls back to pdfminer (as one page).
    """
    pages: list[tuple[int, str]] = []
    # Try PyPDF first (page-accurate)
    try:
        r = PdfReader(io.BytesIO(file_bytes))
        for i, p in enumerate(r.pages):
            pages.append((i, (p.extract_text() or "")))
        if any(t.strip() for _, t in pages):
            return pages
    except Exception:
        pages = []

    # Fallback: pdfminer (whole doc as page 0)
    try:
        whole = extract_text(io.BytesIO(file_bytes)) or ""
        if whole.strip():
            return [(0, whole)]
    except Exception:
        pass
    return []

# --- Chunking WITH page tracking ---
def smart_chunks_by_page(pages: list[tuple[int, str]], target_tokens: int = 900, overlap: int = 150):
    enc = tiktoken.get_encoding("cl100k_base")
    out: list[tuple[int, str]] = []
    for page_idx, page_text in pages:
        toks = enc.encode(page_text or "")
        if not toks:
            continue
        step = max(1, target_tokens - overlap)
        for i in range(0, len(toks), step):
            seg = toks[i:i + target_tokens]
            out.append((page_idx, enc.decode(seg)))
    return out
```

Now, inside **`upsert_document(...)`**, **replace** your current “extract/chunk/embed/insert” block with this page-aware version:

```python
    # Extract text (paged)
    pages = extract_pages_from_pdf(file_bytes)
    joined = "\n".join(t for _, t in pages)
    if len(joined.strip()) < 50:
        supa.table("documents").update({
            "status": "error",
            "processed": False,
            "processing_error": "Scanned PDF not supported in MVP"
        }).eq("id", doc["id"]).execute()
        return doc, 0

    # Chunk + embed (page-aware)
    chunk_tuples = smart_chunks_by_page(pages, target_tokens=900, overlap=150)  # [(page_idx, text)]
    texts = [c for _, c in chunk_tuples]
    embeddings = embed_texts(texts)

    # Pre-summarize each chunk (you already have _summarize_chunk)
    rows = []
    for i, ((page_idx, c), e) in enumerate(zip(chunk_tuples, embeddings)):
        s = _summarize_chunk(c, doc["id"], i)
        rows.append({
            "org_id": org_id,
            "document_id": doc["id"],
            "chunk_index": i,
            "page_index": page_idx,          # <-- store page index
            "content": c,
            "summary": s,
            "token_count": len(c),
            "embedding": e
        })

    if rows:
        supa.table("doc_chunks").insert(rows).execute()
        supa.table("documents").update({"status": "ready", "processed": True}).eq("id", doc["id"]).execute()
    else:
        supa.table("documents").update({"status": "error", "processed": False, "processing_error": "no chunks"}).eq("id", doc["id"]).execute()
```

---

# 2) `lib/rag.py` — include page numbers in citations and deep-link to page

**Replace your entire `lib/rag.py`** with this page-aware, summary-first version:

```python
from openai import OpenAI
from lib.supa import supa, signed_url_for
import os, time
import tiktoken

client = OpenAI()

# ==== CONFIG (env overrideable) ====
CHAT_PRIMARY   = os.getenv("CHAT_PRIMARY", "gpt-4o")         # final answer
EMBED_MODEL    = os.getenv("EMBED_MODEL", "text-embedding-3-small")

MAX_CANDIDATES     = int(os.getenv("MAX_CANDIDATES", "24"))      # retrieved rows (summary-based)
MAX_SUMMARY_TOKENS = int(os.getenv("MAX_SUMMARY_TOKENS", "2400"))# notes budget
MAX_FINAL_TOKENS   = int(os.getenv("MAX_FINAL_TOKENS", "4800"))  # user + notes into final
TEMPERATURE        = float(os.getenv("CHAT_TEMPERATURE", "0.2"))
# ===================================

SYSTEM_PROMPT = (
    "You are Forever Board Member. Answer ONLY from the provided source notes. "
    "Every claim must include an inline citation like [Doc:{document_id}#Chunk:{chunk_index}]. "
    "If the notes are insufficient, say so and ask for the missing document."
)

enc = tiktoken.get_encoding("cl100k_base")
def _toks(s: str) -> int:
    try: return len(enc.encode(s or ""))
    except Exception: return len((s or "").split())

def _retry(fn, tries=4, base=0.6):
    last=None
    for i in range(tries):
        try: return fn()
        except Exception as e:
            last=e; time.sleep(base*(2**i))
    raise last

def _vector(org_id: str, q: str, k: int):
    emb = client.embeddings.create(model=EMBED_MODEL, input=q).data[0].embedding
    try:
        rows = supa.rpc("match_chunks", {
            "query_embedding": emb,
            "match_count": k,
            "org": str(org_id)
        }).execute().data or []
    except Exception:
        rows=[]
    return rows

def _keyword(org_id: str, q: str, k: int):
    terms=[q,"bylaws","policy","rules","minutes","assessment","dues","board","committee","vote","reserved","Open Times","Primary","Juniors","Ladies"]
    seen,out=set(),[]
    for t in terms:
        resp = supa.table("doc_chunks").select("document_id,chunk_index,summary,content").eq("org_id", org_id).ilike("content", f"%{t}%").limit(k).execute().data
        for r in resp or []:
            key=(r["document_id"], r["chunk_index"])
            if key in seen: continue
            seen.add(key); out.append(r)
            if len(out)>=k: break
        if len(out)>=k: break
    return out

def _doc_title_link(doc_id: str):
    d = supa.table("documents").select("title,storage_path").eq("id", doc_id).limit(1).execute().data
    if not d: return ("Document", None)
    return (d[0].get("title") or "Document", signed_url_for(d[0].get("storage_path") or ""))

def _fetch_page_index(doc_id: str, chunk_idx: int) -> int | None:
    try:
        row = supa.table("doc_chunks").select("page_index") \
            .eq("document_id", doc_id).eq("chunk_index", chunk_idx) \
            .limit(1).execute().data
        if row and row[0].get("page_index") is not None:
            return int(row[0]["page_index"])
    except Exception:
        pass
    return None

def answer_question_md(org_id: str, question: str, chat_model: str | None = None):
    # 1) retrieve
    rows = _vector(org_id, question, k=MAX_CANDIDATES)
    if len(rows) < 8:
        rows += _keyword(org_id, question, k=MAX_CANDIDATES)
    # dedupe
    seen=set(); dedup=[]
    for r in rows:
        key=(r.get("document_id"), r.get("chunk_index"))
        if key in seen: continue
        seen.add(key); dedup.append(r)
    rows = dedup[:MAX_CANDIDATES]

    if not rows:
        return ("Insufficient sources found. Upload minutes, bylaws, policies.", [])

    # 2) build notes from pre-summaries, fallback to raw (trimmed)
    notes=[]; total=0; meta=[]
    for r in rows:
        doc_id=r.get("document_id"); ci=r.get("chunk_index")
        title, base_link = _doc_title_link(doc_id)
        page = _fetch_page_index(doc_id, ci)
        link = f"{base_link}#page={page+1}" if (base_link and page is not None) else base_link
        meta.append({"document_id": doc_id, "chunk_index": ci, "title": title, "url": link, "page_index": page})

        s = r.get("summary")
        if not s or len(s)<20:
            content = r.get("content") or ""
            content = (content[:1200] + f" [Doc:{doc_id}#Chunk:{ci}]")
            s = content

        t=_toks(s)
        if total + t > MAX_SUMMARY_TOKENS: break
        notes.append("- " + s.strip())
        total += t

    if not notes:
        return ("Could not build source notes within limits. Refine the question.", meta)

    # 3) final synthesis with strict budget
    preamble = f"QUESTION: {question}\n\nSOURCE NOTES (each ends with its citation):\n"
    body = "\n".join(notes)
    prompt = preamble + body
    while _toks(prompt) > MAX_FINAL_TOKENS and len(notes) > 4:
        notes.pop()
        body = "\n".join(notes)
        prompt = preamble + body

    model = chat_model or CHAT_PRIMARY
    def run():
        resp = client.chat.completions.create(
            model=model,
            temperature=TEMPERATURE,
            messages=[{"role":"system","content":SYSTEM_PROMPT},{"role":"user","content":prompt}],
            max_tokens=700
        )
        return resp.choices[0].message.content
    try:
        answer = _retry(run)
    except Exception:
        # downshift on 429/limits
        model = "gpt-4o-mini"
        answer = _retry(lambda: client.chat.completions.create(
            model=model, temperature=TEMPERATURE,
            messages=[{"role":"system","content":SYSTEM_PROMPT},{"role":"user","content":prompt}],
            max_tokens=600
        ).choices[0].message.content)

    return (answer, meta)
```

---

# 3) `templates/home.html` — show page # in Sources and deep-link

**Find** the JS where you build each Sources `<li>` item (the block that uses `meta.title`/`meta.url`) and **replace that line** with:

```js
const title = meta.title || d;
const pageTxt = (meta.page_index != null) ? ` (p. ${Number(meta.page_index)+1})` : '';
const url = meta.url ? `<a href="${meta.url}" target="_blank">open</a>` : '';
items.push(`<li id="src-${d}-${c}">[${n}] ${title}${pageTxt} — ${url}</li>`);
```

This renders “(p. N)” and makes the link jump to that page (`#page=N`).

---

# 4) Restart the server

```bash
pip install -r requirements.txt
pkill -f gunicorn || true
gunicorn -b 0.0.0.0:8000 app:app
```

---

# 5) Quick verify

* Upload a PDF → ask a question → in **Sources**, click “open”.
* The PDF should open **on the exact page** shown `(p. N)`.

If anything errors, paste the exact traceback and I’ll hand you the one-line fix.
