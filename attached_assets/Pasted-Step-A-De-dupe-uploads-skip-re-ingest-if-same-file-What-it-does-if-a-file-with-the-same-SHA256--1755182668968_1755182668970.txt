Step A — De-dupe uploads (skip re-ingest if same file)
What it does: if a file with the same SHA256 already exists for this org, we reuse the existing document row and don’t re-embed.

Open lib/ingest.py and replace upsert_document(...) with this version:

python
Copy
Edit
def upsert_document(org_id: str, user_id: str, filename: str, file_bytes: bytes, mime_type: str) -> Tuple[dict, int]:
    sha = _sha256_bytes(file_bytes)
    storage_path = f"{org_id}/{sha}/{filename}"

    # ❇️ Check if we already have this exact file for this org
    existing = supa.table("documents")\
        .select("*")\
        .eq("org_id", org_id)\
        .eq("sha256", sha)\
        .limit(1).execute().data
    if existing:
        # Ensure it is uploaded (idempotent) and return existing without re-embedding
        supa.storage.from_(SUPABASE_BUCKET).upload(storage_path, file_bytes, {
            "content-type": mime_type,
            "x-upsert": "true"
        })
        return existing[0], 0

    # Upload to Storage (idempotent)
    supa.storage.from_(SUPABASE_BUCKET).upload(storage_path, file_bytes, {
        "content-type": mime_type,
        "x-upsert": "true"
    })

    # Create/insert document row (compat fields included)
    doc = supa.table("documents").insert({
        "org_id": org_id,
        "created_by": user_id,
        "title": filename,
        "name": filename,
        "filename": filename,
        "storage_path": storage_path,
        "file_path": storage_path,
        "sha256": sha,
        "mime_type": mime_type,
        "size_bytes": len(file_bytes),
        "status": "processing",
        "processed": False,
        "processing_error": None
    }).execute().data[0]

    # Extract text
    text = extract_text_from_pdf(file_bytes)
    if len(text.strip()) < 50:
        supa.table("documents").update({
            "status": "error",
            "processed": False,
            "processing_error": "Scanned PDF not supported in MVP"
        }).eq("id", doc["id"]).execute()
        return doc, 0

    # Chunk + embed
    chunks = smart_chunks(text, target_tokens=900, overlap=150)
    embeddings = embed_texts(chunks)

    rows = []
    for i, (c, e) in enumerate(zip(chunks, embeddings)):
        rows.append({
            "org_id": org_id,
            "document_id": doc["id"],
            "chunk_index": i,
            "content": c,
            "token_count": len(c),
            "embedding": e
        })

    if rows:
        supa.table("doc_chunks").insert(rows).execute()
        supa.table("documents").update({"status": "ready", "processed": True}).eq("id", doc["id"]).execute()
    else:
        supa.table("documents").update({"status": "error", "processed": False, "processing_error": "no chunks"}).eq("id", doc["id"]).execute()

    return doc, len(rows)
Run it: re-upload the same PDF — it should respond with chunks: 0 (reused), and you won’t see a new duplicate in /docs.