# Create new file: lib/memory_synthesis.py

from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from lib.supabase_client import supa
from lib.pattern_engine import PatternRecognitionEngine
import json

class InstitutionalMemorySynthesis:
    def __init__(self):
        self.pattern_engine = PatternRecognitionEngine()
        self.memory_index = {}
        self._build_memory_index()
    
    def recall_complete_history(self, org_id: str, topic: str) -> Dict[str, Any]:
        """Recall EVERYTHING about a topic like a 30-year veteran would"""
        
        # Comprehensive memory retrieval
        memories = {
            'decisions': self._get_all_decisions(org_id, topic),
            'discussions': self._get_all_discussions(org_id, topic),
            'outcomes': self._get_all_outcomes(org_id, topic),
            'lessons': self._get_lessons_learned(org_id, topic),
            'cultural_context': self._get_cultural_context(org_id, topic),
            'key_players': self._get_key_players(org_id, topic),
            'evolution': self._track_evolution(org_id, topic),
            'patterns': self._identify_topic_patterns(org_id, topic),
            'related_topics': self._find_related_topics(org_id, topic)
        }
        
        # Build comprehensive narrative
        narrative = self._build_comprehensive_narrative(memories)
        
        return {
            'comprehensive_history': narrative,
            'key_insights': self._extract_insights(memories),
            'warnings': self._identify_warnings(memories),
            'recommendations': self._generate_recommendations(memories),
            'institutional_wisdom': self._distill_wisdom(memories),
            'cultural_notes': self._extract_cultural_notes(memories),
            'unwritten_rules': self._identify_unwritten_rules(memories)
        }
    
    def answer_as_veteran(self, org_id: str, question: str) -> Dict[str, Any]:
        """Answer exactly as a 30-year board member would"""
        
        # Parse question intent and extract key topics
        intent = self._parse_question_intent(question)
        topics = self._extract_topics(question)
        
        # Gather ALL relevant context
        context = {
            'direct_answers': self._find_direct_answers(org_id, intent),
            'related_decisions': self._find_related_decisions(org_id, topics),
            'cultural_factors': self._consider_cultural_factors(org_id, topics),
            'political_considerations': self._assess_political_landscape(org_id, topics),
            'historical_precedents': self._gather_precedents(org_id, intent),
            'unwritten_rules': self._apply_unwritten_rules(org_id, topics),
            'personal_experiences': self._recall_personal_experiences(org_id, topics),
            'lessons_learned': self._apply_lessons_learned(org_id, topics)
        }
        
        # Synthesize response with veteran's wisdom
        response = self._synthesize_veteran_response(context, question)
        
        return response
    
    def _build_comprehensive_narrative(self, memories: Dict[str, Any]) -> str:
        """Build a complete narrative like a veteran would tell"""
        
        narrative_parts = []
        
        # Opening - set the stage
        if memories['decisions']:
            first_decision = min(memories['decisions'], key=lambda x: x.get('meeting_date', ''))
            narrative_parts.append(f"Looking back to {first_decision.get('meeting_date', 'the early days')}, this topic has quite a history...")
        
        # Evolution over time
        if memories['evolution']:
            narrative_parts.append("\n**How this evolved over the years:**")
            for period, changes in memories['evolution'].items():
                narrative_parts.append(f"- {period}: {changes}")
        
        # Key decisions and their outcomes
        if memories['decisions']:
            narrative_parts.append("\n**Major decisions and what happened:**")
            for decision in memories['decisions']:
                outcome = decision.get('outcome', 'unknown')
                date = decision.get('meeting_date', 'unknown date')
                description = decision.get('description', 'Decision')
                narrative_parts.append(f"- {date}: {description} - {outcome}")
                
                if decision.get('lessons_learned'):
                    narrative_parts.append(f"  *Lesson learned: {decision['lessons_learned']}*")
        
        # Cultural context and unwritten rules
        if memories['cultural_context']:
            narrative_parts.append("\n**What you need to understand about our culture:**")
            for context in memories['cultural_context']:
                narrative_parts.append(f"- {context}")
        
        # Warnings from experience
        if memories['lessons']:
            narrative_parts.append("\n**What I've learned to watch out for:**")
            for lesson in memories['lessons']:
                narrative_parts.append(f"- {lesson}")
        
        return "\n".join(narrative_parts)
    
    def _get_all_decisions(self, org_id: str, topic: str) -> List[Dict]:
        """Get every decision related to the topic"""
        
        # Search in decision registry
        decisions = supa.table("decision_registry") \
            .select("*") \
            .eq("org_id", org_id) \
            .or_(f"description.ilike.%{topic}%,decision_type.ilike.%{topic}%") \
            .order("meeting_date") \
            .execute().data
        
        # Also search in document chunks for decisions
        chunks = supa.table("doc_chunks") \
            .select("*") \
            .eq("org_id", org_id) \
            .ilike("content", f"%{topic}%") \
            .execute().data
        
        # Extract decisions from chunks
        extracted_decisions = self._extract_decisions_from_chunks(chunks, topic)
        
        # Combine and deduplicate
        all_decisions = decisions + extracted_decisions
        return self._deduplicate_decisions(all_decisions)
    
    def _get_cultural_context(self, org_id: str, topic: str) -> List[str]:
        """Extract cultural context and unwritten rules"""
        
        cultural_indicators = [
            "tradition", "always", "never", "typically", "usually",
            "our way", "how we do", "policy", "practice", "custom"
        ]
        
        cultural_contexts = []
        
        # Search for cultural language in documents
        for indicator in cultural_indicators:
            chunks = supa.table("doc_chunks") \
                .select("content") \
                .eq("org_id", org_id) \
                .and_(
                    f"content.ilike.%{topic}%",
                    f"content.ilike.%{indicator}%"
                ) \
                .execute().data
            
            for chunk in chunks:
                # Extract sentences containing cultural context
                sentences = self._extract_cultural_sentences(chunk['content'], topic, indicator)
                cultural_contexts.extend(sentences)
        
        return list(set(cultural_contexts))  # Remove duplicates
    
    def _track_evolution(self, org_id: str, topic: str) -> Dict[str, str]:
        """Track how the topic evolved over time"""
        
        # Get all relevant decisions chronologically
        decisions = self._get_all_decisions(org_id, topic)
        
        if not decisions:
            return {}
        
        evolution = {}
        
        # Group by time periods
        periods = self._group_by_periods(decisions)
        
        for period, period_decisions in periods.items():
            changes = []
            for decision in period_decisions:
                if decision.get('description'):
                    changes.append(decision['description'])
            
            if changes:
                evolution[period] = "; ".join(changes)
        
        return evolution
    
    def _extract_insights(self, memories: Dict[str, Any]) -> List[str]:
        """Extract key insights from all memories"""
        
        insights = []
        
        # Pattern-based insights
        if memories.get('patterns'):
            for pattern_type, patterns in memories['patterns'].items():
                insight = f"Pattern identified: {pattern_type} - {patterns.get('description', '')}"
                insights.append(insight)
        
        # Outcome-based insights
        if memories.get('outcomes'):
            successful_outcomes = [o for o in memories['outcomes'] if o.get('success')]
            failed_outcomes = [o for o in memories['outcomes'] if not o.get('success')]
            
            if successful_outcomes:
                success_factors = self._extract_success_factors(successful_outcomes)
                insights.append(f"Success factors: {', '.join(success_factors)}")
            
            if failed_outcomes:
                failure_factors = self._extract_failure_factors(failed_outcomes)
                insights.append(f"Common failure points: {', '.join(failure_factors)}")
        
        # Decision velocity insights
        if memories.get('decisions'):
            avg_timeline = self._calculate_average_timeline(memories['decisions'])
            insights.append(f"Typical decision timeline: {avg_timeline} days")
        
        return insights
    
    def _synthesize_veteran_response(self, context: Dict[str, Any], question: str) -> Dict[str, Any]:
        """Synthesize response with 30-year veteran wisdom"""
        
        # Build response sections
        response_sections = {
            'direct_answer': self._build_direct_answer(context, question),
            'historical_context': self._build_historical_context(context),
            'precedents': self._build_precedent_analysis(context),
            'cultural_wisdom': self._build_cultural_wisdom(context),
            'practical_advice': self._build_practical_advice(context),
            'warnings': self._build_warnings(context),
            'recommendations': self._build_recommendations(context)
        }
        
        # Combine into veteran-style narrative
        full_response = self._combine_into_narrative(response_sections, question)
        
        return {
            'response': full_response,
            'sections': response_sections,
            'confidence': self._calculate_response_confidence(context),
            'sources': self._compile_sources(context),
            'related_topics': self._suggest_related_topics(context)
        }
    
    def _build_direct_answer(self, context: Dict[str, Any], question: str) -> str:
        """Build the direct answer portion"""
        
        direct_answers = context.get('direct_answers', [])
        
        if not direct_answers:
            return "Based on my experience, I need to tell you that we don't have a clear precedent for this exact situation."
        
        # Find the most relevant answer
        best_answer = max(direct_answers, key=lambda x: x.get('relevance_score', 0))
        
        answer = f"Based on our history, {best_answer.get('answer', '')}"
        
        # Add confidence qualifier like a veteran would
        confidence = best_answer.get('confidence', 0.5)
        if confidence > 0.8:
            answer += " I'm quite certain about this."
        elif confidence > 0.6:
            answer += " This has been pretty consistent over the years."
        else:
            answer += " Though there have been some variations depending on circumstances."
        
        return answer
    
    def _build_historical_context(self, context: Dict[str, Any]) -> str:
        """Build historical context like a veteran would provide"""
        
        related_decisions = context.get('related_decisions', [])
        
        if not related_decisions:
            return ""
        
        # Sort by date
        sorted_decisions = sorted(related_decisions, key=lambda x: x.get('meeting_date', ''))
        
        context_parts = ["Let me give you some background on how we've handled this before:"]
        
        for decision in sorted_decisions[-5:]:  # Last 5 most relevant
            date = decision.get('meeting_date', 'unknown date')
            description = decision.get('description', 'A decision')
            outcome = decision.get('outcome', 'unknown outcome')
            
            context_parts.append(f"- In {date}: {description} - {outcome}")
            
            if decision.get('lessons_learned'):
                context_parts.append(f"  (We learned: {decision['lessons_learned']})")
        
        return "\n".join(context_parts)
    
    def _build_cultural_wisdom(self, context: Dict[str, Any]) -> str:
        """Build cultural wisdom section"""
        
        cultural_factors = context.get('cultural_factors', [])
        unwritten_rules = context.get('unwritten_rules', [])
        
        if not cultural_factors and not unwritten_rules:
            return ""
        
        wisdom_parts = ["Here's what you need to understand about how we operate:"]
        
        for factor in cultural_factors:
            wisdom_parts.append(f"- {factor}")
        
        if unwritten_rules:
            wisdom_parts.append("\nThe unwritten rules around here:")
            for rule in unwritten_rules:
                wisdom_parts.append(f"- {rule}")
        
        return "\n".join(wisdom_parts)


## PART 6: PERFECT RAG IMPLEMENTATION (Week 2, Days 5-7)

### Step 1: Enhanced RAG System with Complete Context

```python
# Create new file: lib/perfect_rag.py

from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer
from lib.supabase_client import supa
from lib.memory_synthesis import InstitutionalMemorySynthesis
from lib.pattern_engine import PatternRecognitionEngine
import openai
import re

class PerfectRAG:
    def __init__(self):
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.memory_synthesis = InstitutionalMemorySynthesis()
        self.pattern_engine = PatternRecognitionEngine()
        
        self.retrieval_strategies = [
            self.exact_match_retrieval,
            self.semantic_retrieval,
            self.pattern_based_retrieval,
            self.temporal_retrieval,
            self.entity_based_retrieval,
            self.cross_reference_retrieval,
            self.institutional_memory_retrieval
        ]
    
    def retrieve_complete_context(self, org_id: str, query: str) -> Dict[str, Any]:
        """Never miss relevant information - comprehensive retrieval"""
        
        # Multi-strategy retrieval
        all_contexts = []
        retrieval_metadata = {}
        
        for strategy in self.retrieval_strategies:
            try:
                contexts = strategy(org_id, query)
                strategy_name = strategy.__name__
                all_contexts.extend(contexts)
                retrieval_metadata[strategy_name] = {
                    'results_count': len(contexts),
                    'top_score': max([c.get('score', 0) for c in contexts]) if contexts else 0
                }
            except Exception as e:
                print(f"Error in {strategy.__name__}: {e}")
                continue
        
        # Deduplicate and rank
        ranked_contexts = self._rank_and_deduplicate_contexts(all_contexts, query)
        
        # Ensure completeness for critical topics
        complete_contexts = self._ensure_completeness(ranked_contexts, query)
        
        # Add cross-references and related information
        enriched_contexts = self._add_cross_references(complete_contexts, org_id, query)
        
        # Build comprehensive context package
        context_package = {
            'primary_contexts': enriched_contexts[:10],  # Top 10 most relevant
            'supporting_contexts': enriched_contexts[10:20],  # Additional context
            'institutional_wisdom': self._get_institutional_wisdom(org_id, query),
            'historical_patterns': self._get_historical_patterns(org_id, query),
            'cultural_context': self._get_cultural_context(org_id, query),
            'related_topics': self._get_related_topics(org_id, query),
            'retrieval_metadata': retrieval_metadata,
            'completeness_score': self._calculate_completeness_score(enriched_contexts, query)
        }
        
        return context_package
    
    def exact_match_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Find exact matches for specific terms, numbers, dates"""
        
        # Extract specific entities from query
        entities = self._extract_query_entities(query)
        results = []
        
        for entity in entities:
            # Search for exact matches
            chunks = supa.table("doc_chunks") \
                .select("*") \
                .eq("org_id", org_id) \
                .ilike("content", f"%{entity}%") \
                .execute().data
            
            for chunk in chunks:
                # Calculate exact match score
                score = self._calculate_exact_match_score(chunk['content'], entity, query)
                if score > 0.5:
                    results.append({
                        'content': chunk['content'],
                        'source': chunk.get('filename', ''),
                        'page': chunk.get('page_number', 0),
                        'score': score,
                        'type': 'exact_match',
                        'matched_entity': entity,
                        'chunk_id': chunk['id']
                    })
        
        return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def semantic_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Semantic similarity search"""
        
        # Get query embedding
        query_embedding = self.embedding_model.encode([query])[0].tolist()
        
        # Search vector database
        chunks = supa.table("doc_chunks") \
            .select("*") \
            .eq("org_id", org_id) \
            .execute().data
        
        results = []
        for chunk in chunks:
            if chunk.get('embedding'):
                # Calculate cosine similarity
                chunk_embedding = chunk['embedding']
                similarity = np.dot(query_embedding, chunk_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)
                )
                
                if similarity > 0.3:  # Relevance threshold
                    results.append({
                        'content': chunk['content'],
                        'source': chunk.get('filename', ''),
                        'page': chunk.get('page_number', 0),
                        'score': float(similarity),
                        'type': 'semantic',
                        'chunk_id': chunk['id']
                    })
        
        return sorted(results, key=lambda x: x['score'], reverse=True)
    
    def pattern_based_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Retrieve based on identified patterns"""
        
        # Identify query type and patterns
        query_patterns = self._identify_query_patterns(query)
        results = []
        
        for pattern in query_patterns:
            # Get pattern-specific information
            pattern_contexts = self.pattern_engine._get_pattern_contexts(org_id, pattern)
            
            for context in pattern_contexts:
                results.append({
                    'content': context['content'],
                    'source': context.get('source', ''),
                    'page': context.get('page', 0),
                    'score': context.get('relevance', 0.5),
                    'type': 'pattern_based',
                    'pattern': pattern,
                    'pattern_confidence': context.get('pattern_confidence', 0.5)
                })
        
        return results
    
    def temporal_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Retrieve based on temporal context"""
        
        # Extract time references from query
        time_references = self._extract_time_references(query)
        results = []
        
        for time_ref in time_references:
            # Search for documents from that time period
            chunks = supa.table("doc_chunks") \
                .select("*") \
                .eq("org_id", org_id) \
                .gte("document_date", time_ref['start_date']) \
                .lte("document_date", time_ref['end_date']) \
                .execute().data
            
            for chunk in chunks:
                # Check content relevance
                content_score = self._calculate_content_relevance(chunk['content'], query)
                if content_score > 0.3:
                    results.append({
                        'content': chunk['content'],
                        'source': chunk.get('filename', ''),
                        'page': chunk.get('page_number', 0),
                        'score': content_score,
                        'type': 'temporal',
                        'time_reference': time_ref,
                        'chunk_id': chunk['id']
                    })
        
        return results
    
    def entity_based_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Retrieve based on entities (people, committees, amounts)"""
        
        entities = self._extract_entities(query)
        results = []
        
        for entity_type, entity_values in entities.items():
            for entity in entity_values:
                # Search for entity mentions
                chunks = supa.table("doc_chunks") \
                    .select("*") \
                    .eq("org_id", org_id) \
                    .ilike("content", f"%{entity}%") \
                    .execute().data
                
                for chunk in chunks:
                    # Verify entity relevance in context
                    relevance = self._verify_entity_relevance(chunk['content'], entity, query)
                    if relevance > 0.4:
                        results.append({
                            'content': chunk['content'],
                            'source': chunk.get('filename', ''),
                            'page': chunk.get('page_number', 0),
                            'score': relevance,
                            'type': 'entity_based',
                            'entity_type': entity_type,
                            'entity_value': entity,
                            'chunk_id': chunk['id']
                        })
        
        return results
    
    def cross_reference_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Find cross-referenced information"""
        
        # Get initial relevant chunks
        initial_chunks = self.semantic_retrieval(org_id, query)[:5]
        results = []
        
        for chunk in initial_chunks:
            # Find cross-references within the chunk
            cross_refs = self._extract_cross_references(chunk['content'])
            
            for ref in cross_refs:
                # Search for referenced content
                ref_chunks = supa.table("doc_chunks") \
                    .select("*") \
                    .eq("org_id", org_id) \
                    .ilike("content", f"%{ref}%") \
                    .execute().data
                
                for ref_chunk in ref_chunks:
                    if ref_chunk['id'] != chunk['chunk_id']:  # Don't include same chunk
                        results.append({
                            'content': ref_chunk['content'],
                            'source': ref_chunk.get('filename', ''),
                            'page': ref_chunk.get('page_number', 0),
                            'score': 0.7,  # High score for cross-references
                            'type': 'cross_reference',
                            'reference': ref,
                            'source_chunk': chunk['chunk_id'],
                            'chunk_id': ref_chunk['id']
                        })
        
        return results
    
    def institutional_memory_retrieval(self, org_id: str, query: str) -> List[Dict[str, Any]]:
        """Retrieve institutional memory and cultural context"""
        
        # Get institutional memory
        memory_recall = self.memory_synthesis.recall_complete_history(org_id, query)
        results = []
        
        # Convert memory components to retrievable contexts
        for memory_type, memory_content in memory_recall.items():
            if isinstance(memory_content, str) and memory_content:
                results.append({
                    'content': memory_content,
                    'source': 'Institutional Memory',
                    'page': 0,
                    'score': 0.8,  # High score for institutional memory
                    'type': 'institutional_memory',
                    'memory_type': memory_type
                })
            elif isinstance(memory_content, list):
                for item in memory_content:
                    if isinstance(item, str) and item:
                        results.append({
                            'content': item,
                            'source': 'Institutional Memory',
                            'page': 0,
                            'score': 0.8,
                            'type': 'institutional_memory',
                            'memory_type': memory_type
                        })
        
        return results
    
    def _ensure_completeness(self, contexts: List[Dict], query: str) -> List[Dict]:
        """Ensure no partial information - get complete sections"""
        
        complete_contexts = []
        seen_chunks = set()
        
        for context in contexts:
            chunk_id = context.get('chunk_id')
            if chunk_id in seen_chunks:
                continue
            
            # Check if this chunk is part of a larger section
            if self._is_partial_section(context['content'], query):
                # Get complete section
                complete_section = self._get_complete_section(chunk_id, query)
                if complete_section:
                    complete_contexts.append(complete_section)
                    # Mark all chunks in this section as seen
                    for section_chunk_id in complete_section.get('section_chunk_ids', []):
                        seen_chunks.add(section_chunk_id)
                else:
                    complete_contexts.append(context)
                    seen_chunks.add(chunk_id)
            else:
                complete_contexts.append(context)
                seen_chunks.add(chunk_id)
        
        return complete_contexts
    
    def _is_partial_section(self, content: str, query: str) -> bool:
        """Check if content appears to be partial"""
        
        # Look for indicators of partial content
        partial_indicators = [
            content.endswith('...'),
            content.startswith('...'),
            re.search(r'\b\d+%, content.strip()),  # Ends with percentage
            re.search(r'\$[\d,]+, content.strip()),  # Ends with dollar amount
            content.count('(') != content.count(')'),  # Unmatched parentheses
            len(content.split()) < 50,  # Very short content
        ]
        
        # Special checks for financial and membership information
        if any(term in query.lower() for term in ['fee', 'cost', 'membership', 'dues', 'rate']):
            if not re.search(r'(?:total|complete|full|entire)', content.lower()):
                return True
        
        return any(partial_indicators)
    
    def _get_complete_section(self, chunk_id: str, query: str) -> Optional[Dict[str, Any]]:
        """Get the complete section containing this chunk"""
        
        # Get the chunk and its siblings
        chunk = supa.table("doc_chunks").select("*").eq("id", chunk_id).execute().data[0]
        
        # Get chunks from same document and nearby pages
        doc_id = chunk.get('document_id')
        page_num = chunk.get('page_number', 0)
        
        sibling_chunks = supa.table("doc_chunks") \
            .select("*") \
            .eq("document_id", doc_id) \
            .gte("page_number", max(1, page_num - 1)) \
            .lte("page_number", page_num + 1) \
            .order("page_number", ascending=True) \
            .order("chunk_order", ascending=True) \
            .execute().data
        
        # Combine chunks to form complete section
        section_content = []
        section_chunk_ids = []
        
        for sibling in sibling_chunks:
            # Check if this chunk is part of the same section
            if self._is_same_section(chunk['content'], sibling['content'], query):
                section_content.append(sibling['content'])
                section_chunk_ids.append(sibling['id'])
        
        if len(section_content) > 1:
            return {
                'content': '\n\n'.join(section_content),
                'source': chunk.get('filename', ''),
                'page': chunk.get('page_number', 0),
                'score': 0.9,  # High score for complete sections
                'type': 'complete_section',
                'section_chunk_ids': section_chunk_ids,
                'is_complete': True
            }
        
        return None
    
    def generate_perfect_response(self, org_id: str, query: str) -> Dict[str, Any]:
        """Generate response with perfect institutional memory"""
        
        # Get comprehensive context
        context_package = self.retrieve_complete_context(org_id, query)
        
        # Build system prompt with veteran persona
        system_prompt = self._build_veteran_system_prompt(context_package)
        
        # Build user prompt with complete context
        user_prompt = self._build_comprehensive_user_prompt(query, context_package)
        
        # Generate response
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        response_text = response.choices[0].message.content
        
        # Enhance response with institutional wisdom
        enhanced_response = self._enhance_with_institutional_wisdom(
            response_text, context_package, query
        )
        
        return {
            'response': enhanced_response,
            'context_used': context_package,
            'confidence': context_package['completeness_score'],
            'institutional_wisdom_applied': True,
            'sources': self._compile_sources(context_package),
            'follow_up_suggestions': self._generate_follow_up_suggestions(query, context_package)
        }
    
    def _build_veteran_system_prompt(self, context_package: Dict[str, Any]) -> str:
        """Build system prompt that embodies 30-year veteran"""
        
        institutional_wisdom = context_package.get('institutional_wisdom', {})
        cultural_context = context_package.get('cultural_context', [])
        
        prompt = """You are the digital embodiment of a 30-year veteran board member with perfect institutional memory. You have witnessed every decision, vote, and discussion in this organization's history.

Your characteristics:
- You recall specific details with precision (exact amounts, dates, vote counts)
- You understand the cultural context and unwritten rules
- You recognize patterns and precedents that others miss
- You provide warnings based on past experiences
- You explain not just what happened, but WHY decisions were made
- You connect current situations to historical precedents
- You understand the political dynamics and member relationships

"""
        
        if institutional_wisdom:
            prompt += "\nInstitutional wisdom you possess:\n"
            for wisdom_type, wisdom_content in institutional_wisdom.items():
                if wisdom_content:
                    prompt += f"- {wisdom_type}: {wisdom_content}\n"
        
        if cultural_context:
            prompt += "\nCultural context you understand:\n"
            for context in cultural_context[:5]:
                prompt += f"- {context}\n"
        
        prompt += """
Response style:
- Start with direct answers based on specific historical precedents
- Provide exact details (amounts, dates, vote counts) when available
- Reference specific meetings and decisions
- Explain the cultural and political context
- Warn about potential issues based on past experience
- Suggest approaches that have worked historically
- Use phrases like "In my experience...", "We've tried this before...", "This reminds me of..."
"""
        
        return prompt
    
    def _build_comprehensive_user_prompt(self, query: str, context_package: Dict[str, Any]) -> str:
        """Build user prompt with all relevant context"""
        
        prompt_parts = [f"Question: {query}\n"]
        
        # Add primary contexts
        if context_package['primary_contexts']:
            prompt_parts.append("Relevant historical information:")
            for i, context in enumerate(context_package['primary_contexts'][:5]):
                prompt_parts.append(f"\n{i+1}. From {context['source']} (page {context['page']}):")
                prompt_parts.append(f"   {context['content']}")
        
        # Add historical patterns
        if context_package.get('historical_patterns'):
            prompt_parts.append("\nHistorical patterns:")
            for pattern in context_package['historical_patterns'][:3]:
                prompt_parts.append(f"- {pattern}")
        
        # Add supporting contexts if needed
        if context_package['supporting_contexts']:
            prompt_parts.append("\nAdditional context:")
            for context in context_package['supporting_contexts'][:3]:
                prompt_parts.append(f"- {context['content'][:200]}...")
        
        return "\n".join(prompt_parts)


## PART 7: VALIDATION AND TESTING SYSTEM (Week 3)

### Step 1: Build Comprehensive Testing Framework

```python
# Create new file: tests/test_institutional_memory.py

import pytest
from typing import Dict, List, Any
from lib.perfect_rag import PerfectRAG
from lib.memory_synthesis import InstitutionalMemorySynthesis
from lib.pattern_engine import PatternRecognitionEngine

class InstitutionalMemoryTester:
    def __init__(self):
        self.perfect_rag = PerfectRAG()
        self.memory_synthesis = InstitutionalMemorySynthesis()
        self.pattern_engine = PatternRecognitionEngine()
        
        # Define test scenarios that a 30-year veteran should handle perfectly
        self.veteran_test_scenarios = self._load_veteran_test_scenarios()
    
    def test_perfect_recall(self, org_id: str) -> Dict[str, Any]:
        """Test that system has perfect recall like a 30-year veteran"""
        
        test_results = {
            'perfect_recall_score': 0,
            'detailed_results': {},
            'failures': [],
            'recommendations': []
        }
        
        for scenario in self.veteran_test_scenarios:
            result = self._test_scenario(org_id, scenario)
            test_results['detailed_results'][scenario['name']] = result
            
            if not result['passed']:
                test_results['failures'].append({
                    'scenario': scenario['name'],
                    'expected': scenario['expected_response'],
                    'actual': result['actual_response'],
                    'issue': result['issue']
                })
        
        # Calculate overall score
        passed_tests = sum(1 for r in test_results['detailed_results'].values() if r['passed'])
        total_tests = len(self.veteran_test_scenarios)
        test_results['perfect_recall_score'] = passed_tests / total_tests
        
        return test_results
    
    def _load_veteran_test_scenarios(self) -> List[Dict[str, Any]]:
        """Load test scenarios that test veteran-level recall"""
        
        return [
            {
                'name': 'Complete Fee Structure Recall',
                'query': 'What are all the membership fees and transfer rules?',
                'expected_elements': [
                    'Foundation membership fees',
                    'Social membership fees', 
                    'Transfer percentages for different years',
                    'Reinstatement rules with all percentages (75%, 50%, 25%)',
                    'Age-based provisions',
                    'Specific dollar amounts'
                ],
                'completeness_threshold': 0.9,
                'accuracy_threshold': 0.95
            },
            {
                'name': 'Historical Decision Patterns',
                'query': 'How have we typically handled vendor contract renewals?',
                'expected_elements': [
                    'Historical vendor decisions',
                    'Approval rates and patterns',
                    'Cost escalation patterns',
                    'Performance evaluation criteria',
                    'Committee involvement patterns'
                ],
                'requires_pattern_recognition': True
            },
            {
                'name': 'Cultural Context Understanding',
                'query': 'What are the unwritten rules about proposing budget increases?',
                'expected_elements': [
                    'Historical context about budget proposals',
                    'Seasonal timing preferences',
                    'Committee preparation requirements',
                    'Member communication patterns',
                    'Success/failure factors'
                ],
                'requires_cultural_wisdom': True
            },
            {
                'name': 'Cross-Reference Accuracy',
                'query': 'Tell me about the 2019 golf course renovation and its financial impact',
                'expected_elements': [
                    'Original proposal details',
                    'Approval process and votes',
                    'Budget vs actual costs',
                    'Implementation timeline',
                    'Member feedback and outcomes',
                    'Lessons learned'
                ],
                'requires_cross_referencing': True
            }
        ]
    
    def test_response_quality(self, org_id: str, query: str) -> Dict[str, Any]:
        """Test response quality against veteran standards"""
        
        # Generate response
        response_data = self.perfect_rag.generate_perfect_response(org_id, query)
        response_text = response_data['response']
        
        quality_metrics = {
            'specificity_score': self._measure_specificity(response_text),
            'accuracy_score': self._measure_accuracy(response_text, response_data['context_used']),
            'completeness_score': self._measure_completeness(response_text, query),
            'veteran_tone_score': self._measure_veteran_tone(response_text),
            'institutional_wisdom_score': self._measure_institutional_wisdom(response_text),
            'cultural_context_score': self._measure_cultural_context(response_text)
        }
        
        # Overall quality score
        quality_metrics['overall_quality'] = sum(quality_metrics.values()) / len(quality_metrics)
        
        return quality_metrics
    
    def _measure_specificity(self, response: str) -> float:
        """Measure how specific the response is (dates, amounts, names)"""
        
        specificity_indicators = [
            len(re.findall(r'\$[\d,]+(?:\.\d{2})?', response)),  # Dollar amounts
            len(re.findall(r'\d+%', response)),  # Percentages
            len(re.findall(r'\b(?:19|20)\d{2}\b', response)),  # Years
            len(re.findall(r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2}', response)),  # Dates
            len(re.findall(r'\d+-\d+', response)),  # Vote counts
        ]
        
        total_indicators = sum(specificity_indicators)
        response_length = len(response.split())
        
        # Score based on density of specific information
        specificity_density = total_indicators / max(response_length / 100, 1)
        return min(1.0, specificity_density / 5)  # Normalize to 0-1
    
    def _measure_veteran_tone(self, response: str) -> float:
        """Measure if response sounds like a 30-year veteran"""
        
        veteran_phrases = [
            'in my experience', 'we\'ve tried this before', 'i recall', 'back in',
            'historically', 'typically we', 'our practice has been', 'i\'ve seen',
            'over the years', 'this reminds me', 'similar situation', 'precedent'
        ]
        
        phrase_count = sum(1 for phrase in veteran_phrases if phrase in response.lower())
        
        # Also check for specific historical references
        historical_references = len(re.findall(r'(?:19|20)\d{2}', response))
        
        # Score based on veteran language usage
        veteran_score = min(1.0, (phrase_count + historical_references) / 5)
        return veteran_score
    
    def run_comprehensive_validation(self, org_id: str) -> Dict[str, Any]:
        """Run comprehensive validation of the entire system"""
        
        validation_results = {
            'perfect_recall_test': self.test_perfect_recall(org_id),
            'pattern_recognition_test': self._test_pattern_recognition(org_id),
            'institutional_memory_test': self._test_institutional_memory(org_id),
            'completeness_test': self._test_completeness(org_id),
            'accuracy_test': self._test_accuracy(org_id),
            'veteran_persona_test': self._test_veteran_persona(org_id)
        }
        
        # Calculate overall system score
        individual_scores = [
            validation_results['perfect_recall_test']['perfect_recall_score'],
            validation_results['pattern_recognition_test']['score'],
            validation_results['institutional_memory_test']['score'],
            validation_results['completeness_test']['score'],
            validation_results['accuracy_test']['score'],
            validation_results['veteran_persona_test']['score']
        ]
        
        validation_results['overall_system_score'] = sum(individual_scores) / len(individual_scores)
        validation_results['ready_for_production'] = validation_results['overall_system_score'] >= 0.9
        
        return validation_results


## IMPLEMENTATION CHECKLIST

### Week 2 Completion Checklist

**Days 1-2: Pattern Recognition Engine ✓**
- [ ] Create pattern recognition database tables
- [ ] Build PatternRecognitionEngine class
- [ ] Implement decision pattern analysis
- [ ] Build outcome prediction system
- [ ] Test pattern recognition with historical data
- [ ] Validate prediction accuracy >70%

**Days 3-4: Institutional Memory Synthesis ✓**
- [ ] Create InstitutionalMemorySynthesis class
- [ ] Build comprehensive memory retrieval
- [ ] Implement veteran-style response synthesis
- [ ] Create cultural context extraction
- [ ] Build historical narrative generation
- [ ] Test memory completeness and accuracy

**Days 5-7: Perfect RAG Implementation ✓**
- [ ] Create PerfectRAG class with multiple retrieval strategies
- [ ] Implement completeness checking and section assembly
- [ ] Build enhanced context packaging
- [ ] Create veteran persona system prompts
- [ ] Implement cross-reference retrieval
- [ ] Test comprehensive response generation

### Week 3 Completion Checklist

**Days 1-2: Validation Framework**
- [ ] Create comprehensive testing framework
- [ ] Build veteran-level test scenarios
- [ ] Implement quality measurement metrics
- [ ] Create accuracy validation system
- [ ] Test perfect recall capabilities
- [ ] Validate 100% information capture

**Days 3-4: System Integration**
- [ ] Integrate all components into unified system
- [ ] Build production-ready API endpoints
- [ ] Implement caching and optimization
- [ ] Create monitoring and logging
- [ ] Build error handling and recovery
- [ ] Test full system performance

**Days 5-7: Demo Preparation**
- [ ] Create killer demo scenarios
- [ ] Build demo-specific interfaces
- [ ] Prepare presentation materials
- [ ] Test demo with real board scenarios
- [ ] Optimize for "wow factor" moments
- [ ] Practice demo flow and timing

## PART 8: PRODUCTION DEPLOYMENT (Week 4)

### Step 1: Production-Ready System Architecture

```python
# Create new file: app.py - Production Flask Application

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from lib.perfect_rag import PerfectRAG
from lib.memory_synthesis import InstitutionalMemorySynthesis
from lib.pattern_engine import PatternRecognitionEngine
from lib.supabase_client import supa
import os
import logging
from datetime import datetime
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Initialize components
perfect_rag = PerfectRAG()
memory_synthesis = InstitutionalMemorySynthesis()
pattern_engine = PatternRecognitionEngine()

@app.route('/')
def index():
    """Main application interface"""
    return render_template('index.html')

@app.route('/api/query', methods=['POST'])
def handle_query():
    """Handle board governance queries with perfect institutional memory"""
    
    try:
        data = request.json
        org_id = data.get('org_id')
        query = data.get('query')
        
        if not org_id or not query:
            return jsonify({'error': 'Missing org_id or query'}), 400
        
        # Log query for analytics
        logger.info(f"Query from {org_id}: {query}")
        
        # Generate perfect response
        start_time = datetime.now()
        response_data = perfect_rag.generate_perfect_response(org_id, query)
        end_time = datetime.now()
        
        # Add performance metadata
        response_data['performance'] = {
            'response_time_ms': int((end_time - start_time).total_seconds() * 1000),
            'timestamp': start_time.isoformat(),
            'version': '2.0-institutional-memory'
        }
        
        # Log successful response
        logger.info(f"Response generated in {response_data['performance']['response_time_ms']}ms")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error processing query: {str(e)}")
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/api/upload-document', methods=['POST'])
def upload_document():
    """Upload and process board documents"""
    
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400
        
        file = request.files['file']
        org_id = request.form.get('org_id')
        
        if not org_id:
            return jsonify({'error': 'Missing org_id'}), 400
        
        # Process document with perfect extraction
        from lib.perfect_extraction import PerfectExtractor
        extractor = PerfectExtractor()
        
        # Save and process file
        filename = f"{org_id}_{datetime.now().isoformat()}_{file.filename}"
        filepath = os.path.join('uploads', filename)
        file.save(filepath)
        
        # Extract with perfect precision
        extraction_result = extractor.extract_complete_document(filepath, org_id)
        
        return jsonify({
            'success': True,
            'document_id': extraction_result['document_id'],
            'chunks_created': extraction_result['chunks_count'],
            'decisions_extracted': extraction_result['decisions_count'],
            'patterns_identified': extraction_result['patterns_count'],
            'processing_time_ms': extraction_result['processing_time_ms']
        })
        
    except Exception as e:
        logger.error(f"Error uploading document: {str(e)}")
        return jsonify({'error': 'Document processing failed'}), 500

@app.route('/api/institutional-insights/<org_id>')
def get_institutional_insights(org_id):
    """Get comprehensive institutional insights"""
    
    try:
        # Generate comprehensive insights
        insights = pattern_engine.generate_governance_insights(org_id)
        
        return jsonify({
            'org_id': org_id,
            'insights': insights,
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error generating insights: {str(e)}")
        return jsonify({'error': 'Failed to generate insights'}), 500

@app.route('/api/decision-analysis', methods=['POST'])
def analyze_decision():
    """Analyze proposed decision with institutional wisdom"""
    
    try:
        data = request.json
        org_id = data.get('org_id')
        proposed_decision = data.get('decision')
        
        if not org_id or not proposed_decision:
            return jsonify({'error': 'Missing org_id or decision data'}), 400
        
        # Analyze with pattern recognition and institutional memory
        from lib.outcome_predictor import DecisionOutcomePredictor
        predictor = DecisionOutcomePredictor()
        
        analysis = predictor.predict_complete_outcome(proposed_decision)
        
        return jsonify({
            'decision_analysis': analysis,
            'generated_at': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Error analyzing decision: {str(e)}")
        return jsonify({'error': 'Decision analysis failed'}), 500

@app.route('/api/health')
def health_check():
    """Health check endpoint"""
    
    try:
        # Test database connection
        result = supa.table("doc_chunks").select("count").execute()
        
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'database': 'connected',
            'version': '2.0-institutional-memory'
        })
        
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

if __name__ == '__main__':
    # Ensure upload directory exists
    os.makedirs('uploads', exist_ok=True)
    
    # Run in production mode
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)), debug=False)